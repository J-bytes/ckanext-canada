import sys
import fileinput
from datetime import datetime
import json
import csv

"""
Gets broken links from url_database.csv and fetches corresponding metadata from 
od-do-canada.jsonl

Arguments:
fileinput - metadata file to be read ('od-do-canada.jsonl')
url_database - url_database report generated by url_database.py

Output:
broken_links_report.csv
"""

broken_links={}
url_database=sys.argv[2]

#Read url_database and put broken links in dict with url as key
file=open(url_database, "r")
reader = csv.reader(file)
#skip head
next(reader)
for line in reader:
    url=line[0]
    date=line[2]
    response=line[3]
    if response == 'not-found':
        broken_links[url]=[line[1],line[2]]
    else:
        continue;
file.close()

#For each resource in each dataset, check if url is in broken links of url_database
#Get metadata
report_data=[]
for dataset in fileinput.input():
    line = json.loads(dataset,'utf-8')
    resources = line["resources"]
    for l in range(len(resources)):
        file_url = resources[l]["url"]
        if file_url in broken_links:
            data = broken_links.pop(file_url)
            report_data.append([file_url,data[0],data[1],
                                line["organization"]["title"],line["title"],line["id"]])
            if len(broken_links)==0:
                break;
            continue;
    if len(broken_links)==0:
        #stop searching when all broken links found
        break;


print("Exporting to csv...")
#Export tp CSV
with open('broken_links_report.csv', "w") as f:
    writer = csv.writer(f)
    writer.writerow(("url", "date","response","organization","title","uuid"))
    for row in report_data:
        writer.writerow(row)
f.close()
print("Done.")

